{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf13328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_json(mode, target_dataset, div_loss = 0.25, idorcodebook = 'codebook'):\n",
    "\n",
    "    codebook_df = pd.read_csv(f'datasets/{target_dataset}/codebooks_{div_loss}.csv')\n",
    "    poi_sequence_df = pd.read_csv(f'datasets/{target_dataset}/data/{mode}.csv')\n",
    "\n",
    "\n",
    "    codebook_df['Codebook'] = codebook_df['Codebook'].apply(eval)\n",
    "\n",
    "    poi_to_codebook = dict(zip(codebook_df['Pid'], codebook_df['Codebook']))\n",
    "\n",
    "    users = []\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for _, row in poi_sequence_df.iterrows():\n",
    "        uid = row['Uid']\n",
    "        poi_sequence = eval(row['Pids'])\n",
    "        time_sequence = eval(row['Times'])\n",
    "        target_time = row['Target_time']\n",
    "        target = row['Target']\n",
    "\n",
    "        if idorcodebook == 'codebook':\n",
    "            embedded_sequence = [\n",
    "                ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[poi])]) + f' at {time_sequence[i]}, ' \n",
    "                if i < len(poi_sequence) - 1 else \n",
    "                ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[poi])]) + f' at {time_sequence[i]}.'\n",
    "                for i, poi in enumerate(poi_sequence)\n",
    "            ]\n",
    "            target_embedding = ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[target])])\n",
    "        \n",
    "        elif idorcodebook == 'id':\n",
    "            embedded_sequence = [\n",
    "                f\"<{poi}>\" + f' at {time_sequence[i]}, ' if i < len(poi_sequence) - 1 else\n",
    "                f\"<{poi}>\" + f' at {time_sequence[i]}.'\n",
    "                for i, poi in enumerate(poi_sequence)\n",
    "            ]\n",
    "            target_embedding = f\"<{target}>\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid idorcodebook value. Use 'codebook' or 'id'.\")\n",
    "\n",
    "        instruction = f\"Here is a record of a user's POI accesses, your task is based on the history to predict the POI that the user is likely to access at the specified time.\"\n",
    "        input = f\"User_{uid} visited: \" + \"\".join(embedded_sequence) + f\" When {target_time} user_{uid} is likely to visit:\"\n",
    "        \n",
    "\n",
    "        sequences.append(input)\n",
    "        targets.append(target_embedding)\n",
    "\n",
    "    semitic_df = pd.DataFrame({\n",
    "        'instruction': instruction,\n",
    "        'input': sequences,\n",
    "        'output': targets\n",
    "    })\n",
    "\n",
    "    json_data = semitic_df.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    with open(f'datasets/{target_dataset}/data/{mode}_{idorcodebook}.json', \"w\") as file:\n",
    "        file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9e27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "save_json('train', 'NYC', 0, 'codebook')\n",
    "save_json('train', 'NYC', 0, 'id')\n",
    "save_json('val', 'NYC', 0, 'codebook')\n",
    "save_json('val', 'NYC', 0, 'id')\n",
    "save_json('test', 'NYC', 0, 'codebook')\n",
    "save_json('test', 'NYC', 0, 'id')\n",
    "\n",
    "save_json('test_all', 'NYC', 0, 'codebook')\n",
    "save_json('test_all', 'NYC', 0, 'id')\n",
    " \"\"\"\n",
    "# save_json('test', 'NYC', 0.25, 'id')\n",
    "# save_json('history', 'NYC', 0.25, 'id')\n",
    "\n",
    "save_json('test', 'NYC', 0.25, 'codebook')\n",
    "save_json('history', 'NYC', 0.25, 'codebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d8f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_json_notime(mode, target_dataset, div_loss = 0.25, idorcodebook = 'codebook'):\n",
    "\n",
    "    codebook_df = pd.read_csv(f'datasets/{target_dataset}/codebooks_{div_loss}.csv')\n",
    "    poi_sequence_df = pd.read_csv(f'datasets/{target_dataset}/data/{mode}.csv')\n",
    "\n",
    "    codebook_df['Codebook'] = codebook_df['Codebook'].apply(eval)\n",
    "\n",
    "    poi_to_codebook = dict(zip(codebook_df['Pid'], codebook_df['Codebook']))\n",
    "\n",
    "    users = []\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for _, row in poi_sequence_df.iterrows():\n",
    "        uid = row['Uid']\n",
    "        poi_sequence = eval(row['Pids'])\n",
    "        time_sequence = eval(row['Times'])\n",
    "        target_time = row['Target_time']\n",
    "        target = row['Target']\n",
    "\n",
    "        if idorcodebook == 'codebook':\n",
    "            embedded_sequence = [\n",
    "                ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[poi])]) + f' ' \n",
    "                if i < len(poi_sequence) - 1 else \n",
    "                ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[poi])]) + f''\n",
    "                for i, poi in enumerate(poi_sequence)\n",
    "            ]\n",
    "            target_embedding = ''.join([f\"<{chr(97 + idx)}_{code}>\" for idx, code in enumerate(poi_to_codebook[target])])\n",
    "        \n",
    "        elif idorcodebook == 'id':\n",
    "            embedded_sequence = [\n",
    "                f\"<{poi}>\" + f' ' if i < len(poi_sequence) - 1 else\n",
    "                f\"<{poi}>\" + f''\n",
    "                for i, poi in enumerate(poi_sequence)\n",
    "            ]\n",
    "            target_embedding = f\"<{target}>\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid idorcodebook value. Use 'codebook' or 'id'.\")\n",
    "\n",
    "        instruction = f\"Here is a record of a user's POI accesses, your task is based on the history to predict the next POI.\"\n",
    "        input = f\"User_{uid} visited: \" + \"\".join(embedded_sequence)\n",
    "        \n",
    "\n",
    "        sequences.append(input)\n",
    "        targets.append(target_embedding)\n",
    "\n",
    "    semitic_df = pd.DataFrame({\n",
    "        'instruction': instruction,\n",
    "        'input': sequences,\n",
    "        'output': targets\n",
    "    })\n",
    "\n",
    "    json_data = semitic_df.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    with open(f'datasets/{target_dataset}/data/{mode}_{idorcodebook}_notime.json', \"w\") as file:\n",
    "        file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ed5477",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msave_json_notime\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNYC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcodebook\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m save_json_notime(\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNYC\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.25\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcodebook\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msave_json_notime\u001b[39m\u001b[34m(mode, target_dataset, div_loss, idorcodebook)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m poi_sequence_df.iterrows():\n\u001b[32m     19\u001b[39m     uid = row[\u001b[33m'\u001b[39m\u001b[33mUid\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     poi_sequence = \u001b[38;5;28meval\u001b[39m(row[\u001b[33m'\u001b[39m\u001b[33mPids\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     21\u001b[39m     time_sequence = \u001b[38;5;28meval\u001b[39m(row[\u001b[33m'\u001b[39m\u001b[33mTimes\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     22\u001b[39m     target_time = row[\u001b[33m'\u001b[39m\u001b[33mTarget_time\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "save_json_notime('train', 'NYC', 0.25, 'codebook')\n",
    "save_json_notime('test', 'NYC', 0.25, 'codebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
